Arguments:

train.py:
  --dataset: <NFMNIST|FMNIST|SYNTH|CIFAR2|CIFAR10>: The dataset used for the experiment.
    (default: 'SYNTH')
  --drop_rate: Dropout rate
    (default: '0.0')
    (a number)
  --exp_name: The name of the experiment
    (default: 'test')
  --job_id: Unique job id assigned by the cluster
    (default: '-1')
    (an integer)
  --learning_rate: Learning rate for SGD
    (default: '0.001')
    (a number)
  --loss: <square|cross_entropy>: The loss used for training the model
    (default: 'square')
  --max_batch_size: Maximum allowed batch size
    (default: '10000')
    (an integer)
  --max_cg_iters: Maximum number of CG / SGD iterations
    (default: '750')
    (an integer)
  --max_ncg_iters: Maximum number of ncg iterations
    (default: '0')
    (an integer)
  --model: <2layerNTK|rf|FullyConnected|Myrtle>: The model used for fitting the data
    (default: '2layerNTK')
  --noise_ind: Index for the noise added to the data
    (default: '0')
    (an integer)
  --num_layers: The number of layers
    (default: '1')
    (an integer)
  --num_units: The number of hidden units
    (default: '5')
    (an integer)
  --reg_index: index for regularization coefficient
    (default: '0')
    (an integer)

absl.flags:
  --flagfile: Insert flag definitions from the given file into the command line.
    (default: '')
  --undefok: comma-separated list of flag names that it is okay to specify on the command line even if the program does not define a flag with that name.  IMPORTANT: flags in this list that have arguments MUST
    use the --flag=value format.
    (default: '')
{'learning_rate': 0.001, 'drop_rate': 0.0, 'reg_index': 0, 'max_batch_size': 10000, 'num_units': 50, 'num_layers': 1, 'job_id': 87176, 'max_cg_iters': 750, 'max_ncg_iters': 0, 'noise_ind': 0, 'loss': 'square', 'exp_name': 'test', 'model': '2layerNTK', 'dataset': 'SYNTH'}
=========
Job ID is 87176
Regularization adjusted to 0.000100
The number of parameters is 6451
Initialized Variables
Iteration: 0, Curr Error: 2.920250, Min Error: 10000.000000
Iteration: 30, Curr Error: 1.476190, Min Error: 2.920250
Iteration: 60, Curr Error: 1.379893, Min Error: 1.476190
Iteration: 90, Curr Error: 1.352048, Min Error: 1.379893
Iteration: 120, Curr Error: 1.342540, Min Error: 1.352048
Iteration: 150, Curr Error: 1.337902, Min Error: 1.342540
Iteration: 180, Curr Error: 1.334136, Min Error: 1.337902
Iteration: 210, Curr Error: 1.331138, Min Error: 1.334136
Iteration: 240, Curr Error: 1.329237, Min Error: 1.331138
Iteration: 270, Curr Error: 1.328372, Min Error: 1.329237
Iteration: 300, Curr Error: 1.327940, Min Error: 1.328372
Iteration: 330, Curr Error: 1.327762, Min Error: 1.327940
Iteration: 360, Curr Error: 1.327588, Min Error: 1.327762
Iteration: 390, Curr Error: 1.327417, Min Error: 1.327588
Iteration: 420, Curr Error: 1.327292, Min Error: 1.327417
Iteration: 450, Curr Error: 1.327185, Min Error: 1.327292
Iteration: 480, Curr Error: 1.327116, Min Error: 1.327185
Iteration: 510, Curr Error: 1.327086, Min Error: 1.327116
Iteration: 540, Curr Error: 1.327030, Min Error: 1.327086
Iteration: 570, Curr Error: 1.326978, Min Error: 1.327030
Iteration: 600, Curr Error: 1.326934, Min Error: 1.326978
Iteration: 630, Curr Error: 1.326909, Min Error: 1.326934
Initial CG took 69.272516 seconds
Loss for the final CG iteration is 1.326893
Loss after initial CG is 1.326893
Tolerance achieved
Regularization Loss is 0.005586
Train loss is 1.321307
Test loss is 4.365883
{'learning_rate': 0.001, 'drop_rate': 0.0, 'reg_index': 0, 'max_batch_size': 10000, 'num_units': 50, 'num_layers': 1, 'job_id': 87176, 'max_cg_iters': 750, 'max_ncg_iters': 0, 'noise_ind': 0, 'loss': 'square', 'exp_name': 'test', 'model': '2layerNTK', 'dataset': 'SYNTH'}

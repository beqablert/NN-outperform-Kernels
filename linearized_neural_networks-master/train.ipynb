{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1e9acab7aad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The Experiment is Finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1e9acab7aad6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munknown_flags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0m\u001b[1;32m    670\u001b[0m           name, value, suggestions=suggestions)\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code uses TensorFlow to fit NNs, CNNs,\n",
    "Random Feature Models and Neural Tangent Models to the data.\n",
    "\n",
    "The code was written for python/2.7.13 and TensorFlow v 1.12\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, './linear_algebra/')\n",
    "from optimization_utils import OptimizationUtils\n",
    "sys.path.insert(0, './model_zoo/')\n",
    "from neural_networks import TwoLayerReluNT, RF, FullyConnected, Myrtle\n",
    "\n",
    "from rf_optimizer import RF_Optimizer\n",
    "from preprocess import prep_data\n",
    "\n",
    "import scipy.optimize as sco\n",
    "import scipy.linalg as scl\n",
    "import scipy.sparse as ss\n",
    "import time\n",
    "import os \n",
    "import pickle as pickle\n",
    "\n",
    "flags = tf.compat.v1.flags\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(flags.FLAGS)\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Learning rate for SGD')\n",
    "flags.DEFINE_float('drop_rate', 0.0, 'Dropout rate')\n",
    "flags.DEFINE_integer('reg_index', 0, 'index for regularization coefficient')\n",
    "flags.DEFINE_integer('max_batch_size', 10000, 'Maximum allowed batch size')\n",
    "flags.DEFINE_integer('num_units', 4096, 'The number of hidden units')\n",
    "flags.DEFINE_integer('num_layers', 1, 'The number of layers')\n",
    "flags.DEFINE_integer('job_id', -1, 'Unique job id assigned by the cluster')\n",
    "\n",
    "flags.DEFINE_integer('max_cg_iters', 750, 'Maximum number of CG / SGD iterations')\n",
    "flags.DEFINE_integer('max_ncg_iters', 0, 'Maximum number of ncg iterations')\n",
    "\n",
    "flags.DEFINE_integer('noise_ind', 0, 'Index for the noise added to the data')\n",
    "flags.DEFINE_enum('loss', 'square', ['square', 'cross_entropy'], 'The loss used for training the model')\n",
    "flags.DEFINE_string('exp_name', 'test', 'The name of the experiment')\n",
    "flags.DEFINE_enum('model', '2layerNTK', ['2layerNTK', 'rf', 'FullyConnected', 'Myrtle'], 'The model used for fitting the data')\n",
    "flags.DEFINE_enum('dataset', 'SYNTH', ['NFMNIST', 'FMNIST', 'SYNTH', 'CIFAR2', 'CIFAR10'], 'The dataset used for the experiment.')\n",
    "\n",
    "#FLAGS = flags.FLAGS\n",
    "\n",
    "class Experiment(object):\n",
    "\tdef __init__(self):\n",
    "\t\tFLAGS = tf.compat.v1.flags.FLAGS\n",
    "\t\tprint(FLAGS.__getattr__(\"dataset\"))\n",
    "\t\tself._dataset = FLAGS.dataset\n",
    "\t\tself._model_name = FLAGS.model\n",
    "\t\tself._params = {}\t\t\n",
    "\t\tif FLAGS.job_id < 0:\n",
    "\t\t\tFLAGS.job_id = np.random.randint(0, 10 ** 5)\n",
    "\t\tself._mkdir(FLAGS.exp_name, FLAGS.model, FLAGS.job_id, FLAGS.num_units, FLAGS.dataset, FLAGS)\n",
    "\t\tprint('Job ID is %d'%(FLAGS.job_id), file=self._file)\n",
    "\n",
    "\t\t# process flags to form the directory and choose the model \n",
    "\t\tself._m = FLAGS.num_layers\n",
    "\t\tmodel_fn = self._choose_model(FLAGS.model, FLAGS.reg_index)\t\t\n",
    "\t\tself._import_data(FLAGS.dataset, FLAGS.model, FLAGS.noise_ind)\n",
    "\n",
    "\t\tself._params['loss_type'] = FLAGS.loss\t\t\n",
    "\t\tif FLAGS.model in ['FullyConnected']:\n",
    "\t\t\tself._params['dropout'] = FLAGS.drop_rate\n",
    "\t\telse:\n",
    "\t\t\tself._params['dropout'] = None\n",
    "\t\tmax_batch_size = FLAGS.max_batch_size\n",
    "\n",
    "\t\t# dataset specific hyper-parameters\n",
    "\t\tif FLAGS.dataset == 'SYNTH':\n",
    "\t\t\tself._params['mean'] = 0.0\t\t\t    \t    \t\t\n",
    "\t\t\tself._params['expandFinal'] = False\n",
    "\t\t\tself._params['num_classes'] = 1\n",
    "\t\t\tself._params['max_class'] = 1\t\t\t\n",
    "\t\telif FLAGS.dataset == 'CIFAR2':\n",
    "\t\t\tself._params['mean'] = 0.5\n",
    "\t\t\tself._params['expandFinal'] = False\n",
    "\t\t\tself._params['num_classes'] = 1\n",
    "\t\t\tself._params['max_class'] = 1\t\t\t\n",
    "\t\t\tmax_batch_size = 2000\n",
    "\t\telse:\n",
    "\t\t\tself._params['mean'] = 0.0\n",
    "\t\t\tself._params['expandFinal'] = True\n",
    "\t\t\tself._params['num_classes'] = 10\n",
    "\t\t\tself._params['max_class'] = 9\t\n",
    "\t\t\tmax_batch_size = np.minimum(10000, max_batch_size)\n",
    "\t\t\t\n",
    "\t\t# Hyper-parameters\n",
    "\t\tself._n = self._X.shape[0]\n",
    "\t\tself._d = self._X.shape[1]\t\t\n",
    "\t\tself._N = FLAGS.num_units\t\t\n",
    "\t\tself._tol = 2e-2\t\t\n",
    "\t\tgraph = tf.Graph()\n",
    "\t\ttf.reset_default_graph()\n",
    "\t\twith graph.as_default():\n",
    "\t\t\ttf.set_random_seed(91)\n",
    "\t\t\tself._batch_size = tf.placeholder(tf.int64, shape=[])\n",
    "\t\t\tx, y, iterator = self._data_pipeline(FLAGS.model)\t\t\t\n",
    "\t\t\t# Note that when training CNNs, 'num_hidden' is used for\n",
    "\t\t\t# the number of channcel\n",
    "\t\t\tself._params['num_hiddens'] = self._N\n",
    "\t\t\tself._params['num_layers'] = self._m\n",
    "\t\t\tself._params['feature_dims'] = self._d\t\t\t\n",
    "\t\t\tself._params['train_vars'] = True\n",
    "\t\t\tself._params['filter_size'] = 3\n",
    "\t\t\t# For the scenario where CG is not available\n",
    "\t\t\tif self._model_name in ['rf', '2layerNTK']:\n",
    "\t\t\t\tself._params['optimizer'] = 'adam'\n",
    "\t\t\telse:\n",
    "\t\t\t\tself._params['optimizer'] = 'mom'\t\t\t\t\n",
    "\t\t\tself._params['include_bias'] = True\n",
    "\t\t\tself._params['reg_param'] = self._model_reg\t\t\t\n",
    "\t\t\tprint('Model Reg is %f'%(self._model_reg))\n",
    "\t\t\tmodel = model_fn(x, y, self._params)\t\t\t\n",
    "\t\t\tif FLAGS.model == 'rf':\n",
    "\t\t\t\tself._optim_tool = None \n",
    "\t\t\t\tShapeList = [np.prod(g.shape) for g in model._variables]\n",
    "\t\t\t\tself._num_params = np.int(np.sum(ShapeList))\n",
    "\t\t\telse:\n",
    "\t\t\t\tself._optim_tool = OptimizationUtils(model._loss, model._preds, model._variables, y, self._optim_reg, self._n)\n",
    "\t\t\t\tself._num_params = self._optim_tool.num_params()\n",
    "\t\t\tprint('The number of parameters is %d'%(self._num_params), file=self._file)\n",
    "\t\t\t# Train the network\n",
    "\t\t\twith tf.Session() as sess:\n",
    "\t\t\t\tsess.run(tf.global_variables_initializer())\n",
    "\t\t\t\tif FLAGS.model == 'rf':\t\n",
    "\t\t\t\t\tnp.random.seed(1990)\t\t\t\t\t\n",
    "\t\t\t\t\tW0 = np.random.normal(size=(self._d, self._N))\n",
    "\t\t\t\t\tnorms = np.linalg.norm(W0, axis=0, keepdims=True)\n",
    "\t\t\t\t\tW0 = W0 / norms\n",
    "\t\t\t\t\t# Forming NTK approx\n",
    "\t\t\t\t\tsess.run(model._rf_assign, {model._tph: W0})\n",
    "\t\t\t\t\tdel W0, norms\t\n",
    "\t\t\t\tprint('Initialized Variables', file=self._file)\n",
    "\t\t\t\tsave_dict = self._optim_fun(model, sess, iterator, FLAGS.max_cg_iters, FLAGS.max_ncg_iters, FLAGS.learning_rate, max_batch_size)\n",
    "\t\t\t\tsave_dict['train_loss'], save_dict['train_accuracy'] = self._record_stats(sess, 'train', model, iterator, max_batch_size)\n",
    "\t\t\t\tsave_dict['test_loss'], save_dict['test_accuracy'] = self._record_stats(sess, 'test', model, iterator, max_batch_size)\n",
    "\t\tself._save_results(save_dict, FLAGS)\n",
    "\n",
    "\tdef _import_data(self, dataset, model, dim):\n",
    "\t\tself._X, self._Y, self._Xtest, self._Ytest = prep_data(dataset, model == 'CNN', dim)\t\t\n",
    "\n",
    "\tdef _save_results(self, save_dict, exp_flag):\n",
    "\t\tsave_dict['num_param'] = self._num_params\t\t\n",
    "\t\tsave_dict['reg_coeff'] = (self._optim_reg + self._model_reg)\n",
    "\t\tprint('Regularization Loss is %f'%(save_dict['reg_loss']), file=self._file)\t\t\n",
    "\t\tprint('Train loss is %f'%(save_dict['train_loss']), file=self._file)            \t\t\n",
    "\t\tprint('Test loss is %f'%(save_dict['test_loss']), file=self._file)\t\t\n",
    "\t\ttemp = exp_flag.flag_values_dict()\n",
    "\t\tprint(temp, file=self._file)\n",
    "\t\tself._file.close()\n",
    "\t\tfor key in temp.keys():\n",
    "\t\t\tsave_dict[key] = temp[key]\n",
    "\t\tfileName = self._directory + \"/\" + 'stats.pkl'\n",
    "\t\twith open(fileName, 'wb') as output:    \n",
    "\t\t\tpickle.dump(save_dict, output, -1)\n",
    "\n",
    "\tdef _data_pipeline(self, model):\n",
    "\t\tif self._model_name in ['Myrtle']:\n",
    "\t\t\tself._features_placeholder = tf.placeholder(self._X.dtype, (None, self._X.shape[1], self._X.shape[2], self._X.shape[3]))\n",
    "\t\telse:\n",
    "\t\t\tself._features_placeholder = tf.placeholder(self._X.dtype, (None, self._X.shape[1]))\n",
    "\t\tself._labels_placeholder = tf.placeholder(self._Y.dtype, (None, 1))\n",
    "\t\tif self._model_name in ['rf', '2layerNTK']:\n",
    "\t\t\tdataset = tf.data.Dataset.from_tensor_slices((self._features_placeholder, self._labels_placeholder)).repeat(1).batch(self._batch_size, drop_remainder=True)\n",
    "\t\telse:\n",
    "\t\t\tdataset = tf.data.Dataset.from_tensor_slices((self._features_placeholder, self._labels_placeholder)).shuffle(128).repeat(1).batch(self._batch_size, drop_remainder=True)\n",
    "\t\tdataset = dataset.prefetch(buffer_size=16)\n",
    "\t\titerator = dataset.make_initializable_iterator()\n",
    "\t\tx, y = iterator.get_next()\n",
    "\t\treturn (x, y, iterator)\n",
    "\n",
    "\tdef _record_stats(self, sess, mode, model, iterator, batch_size):\n",
    "\t\t\"\"\"This function calculates training / test loss and accuracies over the full dataset.\"\"\"\t\t\n",
    "\t\tacc_list = []\n",
    "\t\tloss_list = []\n",
    "\t\tdrop_rate = 0.0\n",
    "\t\tif mode == 'train':\n",
    "\t\t\tbatch_size = min(batch_size, self._Y.shape[0])\n",
    "\t\t\tsess_dict = {self._features_placeholder: self._X, self._labels_placeholder: self._Y, self._batch_size: batch_size}\n",
    "\t\t\tloss_measure = model._loss\n",
    "\t\t\tdrop_rate = self._params['dropout']\n",
    "\t\telse:\n",
    "\t\t\tbatch_size = 100 #min(batch_size, self._Ytest.shape[0])\n",
    "\t\t\tsess_dict = {self._features_placeholder: self._Xtest, self._labels_placeholder: self._Ytest, self._batch_size: batch_size}\t\t\t\n",
    "\t\t\tloss_measure = model._pure_loss\n",
    "\t\t\tdrop_rate = 0.0\n",
    "\t\tsess.run(iterator.initializer, feed_dict=sess_dict)\n",
    "\t\tend_of_data = False\n",
    "\t\twhile not end_of_data:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif self._params['dropout'] is None:\n",
    "\t\t\t\t\ttemp_loss, temp_accuracy = sess.run([loss_measure, model._accuracy])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp_loss, temp_accuracy = sess.run([loss_measure, model._accuracy], feed_dict={model._drop_rate: drop_rate})\n",
    "\t\t\t\tloss_list.append(temp_loss)\n",
    "\t\t\t\tacc_list.append(temp_accuracy)\n",
    "\t\t\texcept tf.errors.OutOfRangeError:\n",
    "\t\t\t\tend_of_data = True  \n",
    "\t\treturn (np.mean(loss_list), np.mean(acc_list))\n",
    "\n",
    "\tdef _rf_optimizer(self, model, sess, iterator, num_iters, ncg_iters, lr, max_batch_size):\n",
    "\t\t\"\"\" This function uses the RF_Optimizer class to train large RF models with CG. While both _rf_optimizer\n",
    "\t\t and _ls_optimizer can be used to train the models with CG, RF_Optimizer is more optimized and hence more\n",
    "\t\t suitable for training large RF models. \"\"\"\n",
    "\n",
    "\t\tsess_dict = {self._features_placeholder: self._X, self._labels_placeholder: self._Y, self._batch_size: max_batch_size}\n",
    "\t\tprint('Batch Size is %d'%(max_batch_size), file=self._file)\n",
    "\t\tn = self._n\n",
    "\t\tself._optim_tool = RF_Optimizer(model, sess, iterator.initializer, sess_dict, self._optim_reg, n, self._labels_placeholder, True)\n",
    "\t\tprint('Optimizer Created', file=self._file)\n",
    "\t\txerr = np.zeros((self._num_params,))    \t\t\t\t\t\t\n",
    "\t\tdef matvec(x):\n",
    "\t\t\treturn self._optim_tool.Hv(x)\n",
    "\t    \n",
    "\t\trep_stats = np.zeros((2,))\n",
    "\t\trep_stats[1] = 10000\n",
    "\t\tcg_error_hist = []\n",
    "\t\tdef report(xk):                \n",
    "\t\t\tif rep_stats[0] % 30 == 0:\n",
    "\t\t\t\terror = self._optim_tool.fun(xk)\n",
    "\t\t\t\tcg_error_hist.append(error)\n",
    "\t\t\t\tprint('Iteration: %d, Curr Error: %f, Min Error: %f'%(rep_stats[0], error, rep_stats[1]), file=self._file)\n",
    "\t\t\t\tif error < rep_stats[1]:\n",
    "\t\t\t\t\trep_stats[1] = error \n",
    "\t\t\t\t\txerr[:] = np.copy(xk)\n",
    "\t\t\trep_stats[0] += 1 \n",
    "\n",
    "\t\tb = self._optim_tool.Atx() * 2.0 / np.sqrt(n + 0.0)\n",
    "\t\tprint('b vector computed', file=self._file)\n",
    "\t\tprint(np.linalg.norm(b), file=self._file)\n",
    "\t\t\n",
    "\t\tt1 = time.time()\n",
    "\t\tlin_op = ss.linalg.LinearOperator((self._num_params, self._num_params), matvec=matvec, rmatvec=matvec, dtype=np.float32)\n",
    "\t\tcg = ss.linalg.cg(lin_op, b, maxiter=num_iters, atol=1e-3, tol=1e-4, callback=report)\n",
    "\t\tt2 = time.time()  \n",
    "\t\tprint('Initial CG took %f seconds'%(t2 - t1), file=self._file)\n",
    "\t\tx0 = np.copy(cg[0])\n",
    "\t\tfirst_stage_loss = self._optim_tool.fun(x0)    \n",
    "\t\tprint('Loss for the final CG iteration is %f'%(first_stage_loss), file=self._file)\n",
    "\t\tprint('Loss after initial CG is %f'%(first_stage_loss), file=self._file)\n",
    "\t\tsave_dict = {}\n",
    "\t\tsave_dict['cg_hist'] = cg_error_hist\n",
    "\t\tsave_dict['res_norm'] = None\n",
    "\t\tsave_dict['Message'] = 'Optimization terminated after CG.'\n",
    "\t\tsave_dict['x'] = x0\n",
    "\t\tsave_dict['reg_loss'] = self._optim_reg * np.linalg.norm(save_dict['x']) ** 2\n",
    "\t\treturn save_dict\n",
    "\n",
    "\tdef _ls_optimizer(self, model, sess, iterator, num_iters, ncg_iters, lr, max_batch_size):\n",
    "\t\t\"\"\" This function uses TensorFlow capabilities to train a TwoLayerReluNT model with CG.\"\"\"\n",
    "\t\txerr = np.zeros((self._num_params,))    \t\t\n",
    "\t\tsess_dict = {self._features_placeholder: self._X, self._labels_placeholder: self._Y, self._batch_size: max_batch_size}\n",
    "\t\tn = self._n\n",
    "\t\tdef fun(x):\n",
    "\t\t\tsess.run(iterator.initializer, feed_dict=sess_dict)\n",
    "\t\t\tval, reg = self._optim_tool.loss(x, sess)\n",
    "\t\t\treturn val + reg\t\t\t\n",
    "\t\t\t\n",
    "\t\tdef Atx(x):\n",
    "\t        # Note x is n dimensional\n",
    "\t\t\tassert len(x) == n\n",
    "\t\t\tnew_dict = {self._features_placeholder: self._X, self._labels_placeholder: x.reshape(n, 1), self._batch_size: max_batch_size}    \n",
    "\t\t\tsess.run(iterator.initializer, feed_dict=new_dict)\n",
    "\t\t\tatx = self._optim_tool.ATx(sess)\t        \n",
    "\t\t\treturn atx[:, 0]\n",
    "\n",
    "\t\tdef hv(x, p):\n",
    "\t\t\t\"\"\"Warning: This function assume that the Hessian is constant.\"\"\"        \n",
    "\t\t\tsess.run(iterator.initializer, feed_dict=sess_dict)\n",
    "\t\t\tvec = self._optim_tool.Hv(None, p, sess)\n",
    "\t\t\treturn vec[:, 0]\n",
    "\n",
    "\t\tdef grad(x):        \n",
    "\t\t\tsess.run(iterator.initializer, feed_dict=sess_dict)\n",
    "\t\t\tgradient = self._optim_tool.gradient(x, sess)\n",
    "\t\t\treturn gradient[:, 0]\n",
    "\t    \n",
    "\t\tdef matvec(x):\n",
    "\t\t\treturn hv(None, x)\n",
    "\t    \n",
    "\t\trep_stats = np.zeros((2,))\n",
    "\t\trep_stats[1] = 10000\n",
    "\t\tcg_error_hist = []\n",
    "\t\tdef report(xk):                \n",
    "\t\t\tif rep_stats[0] % 30 == 0:\n",
    "\t\t\t\terror = fun(xk)\n",
    "\t\t\t\tcg_error_hist.append(error)\n",
    "\t\t\t\tprint('Iteration: %d, Curr Error: %f, Min Error: %f'%(rep_stats[0], error, rep_stats[1]), file=self._file)\n",
    "\t\t\t\tif error < rep_stats[1]:\n",
    "\t\t\t\t\trep_stats[1] = error \n",
    "\t\t\t\t\txerr[:] = np.copy(xk)\n",
    "\t\t\trep_stats[0] += 1 \n",
    "\n",
    "\t\tb = Atx(self._Y[:, 0]) * 2.0 / np.sqrt(n + 0.0)\n",
    "\t\tt1 = time.time()\n",
    "\t\tlin_op = ss.linalg.LinearOperator((self._num_params, self._num_params), matvec=matvec, rmatvec=matvec, dtype=np.float32)                \n",
    "\t\t#cg = ss.linalg.cg(lin_op, b, maxiter=num_iters, atol=1e-4, tol=1e-5, callback=report)\n",
    "\t\tcg = ss.linalg.cg(lin_op, b, maxiter=num_iters, atol=1e-3, tol=1e-4, callback=report)\n",
    "\t\tt2 = time.time()  \n",
    "\t\tprint('Initial CG took %f seconds'%(t2 - t1), file=self._file)\n",
    "\t\tx0 = np.copy(cg[0])\n",
    "\t\tfirst_stage_loss = fun(x0)    \n",
    "\t\tprint('Loss for the final CG iteration is %f'%(first_stage_loss), file=self._file)\n",
    "\t\tif first_stage_loss > rep_stats[1]:\n",
    "\t\t\tfirst_stage_loss = rep_stats[1]\n",
    "\t\t\tx0 = np.copy(xerr)\n",
    "\t\tprint('Loss after initial CG is %f'%(first_stage_loss), file=self._file)\n",
    "\t\tsave_dict = {}\n",
    "\t\tsave_dict['cg_hist'] = cg_error_hist\n",
    "\t\tsave_dict['res_norm'] = None\n",
    "\t\tsave_dict['x0'] = x0\n",
    "\t\tif first_stage_loss < self._tol or ncg_iters == 0:\n",
    "\t\t\tprint('Tolerance achieved', file=self._file)\n",
    "\t\t\tsave_dict['Message'] = 'Optimization terminated after CG.'\n",
    "\t\t\tsave_dict['x'] = x0\n",
    "\t\t\tsave_dict['grad'] = np.linalg.norm(grad(x0))\n",
    "\t\telse:\n",
    "\t\t\tt1 = time.time()\n",
    "\t\t\toptim_obj = sco.minimize(fun, x0, method='trust-ncg', jac=grad, hessp=hv, options={'disp':True, 'maxiter': ncg_iters})                \n",
    "\t\t\tt2 = time.time()\n",
    "\t\t\tprint('trust-ncg took %f seconds'%(t2 - t1), file=self._file)        \n",
    "\t\t\tprint('Loss after trust-ncg is %f'%(fun(optim_obj['x'])), file=self._file)\n",
    "\t\t\tsave_dict['Message'] = optim_obj['message']\n",
    "\t\t\tsave_dict['x'] = optim_obj['x']\n",
    "\t\t\tsave_dict['grad'] = np.linalg.norm(optim_obj['jac'])\n",
    "\t\tsave_dict['reg_loss'] = self._optim_tool._reg_param * np.linalg.norm(save_dict['x']) ** 2\n",
    "\t\treturn save_dict\n",
    "\n",
    "\tdef _fc_optimizer(self, model, sess, iterator, sgd_iters, ncg_iters, lr, max_batch_size):\n",
    "\t\t\"\"\" This function is used for training the models with first-order methods.\"\"\"\n",
    "\t\tdef lr_fun(t, max_iters):\n",
    "\t\t\tpi = 3.141592\n",
    "\t\t\twarm_up = 15\n",
    "\t\t\tif t < warm_up:\n",
    "\t\t\t\treturn lr * 2 * t / (warm_up + 0.0)\n",
    "\t\t\telse:          \n",
    "\t\t\t\tn = t - warm_up\n",
    "\t\t\t\tN = max_iters - warm_up + 0.0\n",
    "\t\t\t\treturn np.maximum(lr * (1 + np.cos(n * pi / N)), lr / 15.0)\n",
    "\n",
    "\t\tdef batch_fun(t):\n",
    "\t\t\tif self._model_name in ['rf', '2layerNTK']:\n",
    "\t\t\t\treturn max_batch_size     \n",
    "\t\t\tif self._model_name in ['Myrtle']:\n",
    "\t\t\t\treturn 128\n",
    "\t\t\tif self._dataset == 'CIFAR2':\n",
    "\t\t\t\treturn 250\n",
    "\t\t\tif self._dataset in ['FMNIST', 'NFMNIST']:\n",
    "\t\t\t\tif t < 15:                \n",
    "\t\t\t\t\treturn 500\n",
    "\t\t\t\treturn 1000\n",
    "\t\t\tif t < 15:                \n",
    "\t\t\t\treturn 512\n",
    "\t\t\treturn 1024\n",
    "\t\t\t\n",
    "\t\tepoch = 0\n",
    "\t\ttrain = np.zeros((sgd_iters, 3))\n",
    "\t\ttest = np.zeros((sgd_iters, 2))\t\t\n",
    "\t\tsess_dict = {self._features_placeholder: self._X, self._labels_placeholder: self._Y, self._batch_size: batch_fun(epoch)}\n",
    "\t\tsess.run(iterator.initializer, feed_dict=sess_dict)\n",
    "\t\terrs = []\n",
    "\t\taccs = []\n",
    "\t\tregLoss = []\n",
    "\t\twhile epoch < sgd_iters:\n",
    "\t\t\ttry:                \n",
    "\t\t\t\t_, err, acc, regL = sess.run([model._train_op, model._loss, model._accuracy, model._reg_loss],\\\n",
    "\t\t\t\tfeed_dict={model._lr: lr_fun(epoch, sgd_iters), model._drop_rate: self._params['dropout']})\n",
    "\t\t\t\terrs.append(err)\n",
    "\t\t\t\taccs.append(acc)\n",
    "\t\t\t\tregLoss.append(regL)\n",
    "\t\t\texcept tf.errors.OutOfRangeError:          \n",
    "\t\t\t\ttrain[epoch, 0] = np.mean(errs)\n",
    "\t\t\t\ttrain[epoch, 1] = np.mean(accs)\n",
    "\t\t\t\ttrain[epoch, 2] = np.mean(regLoss)\n",
    "\t\t\t\terrs = []\n",
    "\t\t\t\taccs = []\n",
    "\t\t\t\tregLoss = []\n",
    "\t\t\t\tsess_dict = {self._features_placeholder: self._Xtest, self._labels_placeholder: self._Ytest, self._batch_size: max_batch_size}\n",
    "\t\t\t\tsess.run(iterator.initializer, feed_dict=sess_dict)\t                \t                \n",
    "\t\t\t\ttest[epoch, 0], test[epoch, 1] = sess.run([model._pure_loss, model._accuracy], feed_dict={model._drop_rate: 0.0})\n",
    "\t\t\t\tprint('epoch: %d, Train loss: %f, Test loss: %f, Train Accuracy: %f, Test Accuracy: %f'%(\\\n",
    "\t\t\t\t\tepoch, train[epoch, 0], test[epoch, 0], train[epoch, 1], test[epoch, 1]), file=self._file)\n",
    "\t\t\t\tepoch += 1\n",
    "\t\t\t\tsess_dict = {self._features_placeholder: self._X, self._labels_placeholder: self._Y, self._batch_size: batch_fun(epoch)}\n",
    "\t\t\t\tsess.run(iterator.initializer, feed_dict=sess_dict)\n",
    "\t\tx0 = sess.run(model._variables)\n",
    "\t\tsave_dict = {}\n",
    "\t\tsave_dict['train_hist'] = train\n",
    "\t\tsave_dict['test_hist'] = test\t\n",
    "\t\tsave_dict['x0'] = x0\t\t\t\t\n",
    "\t\t# Saving final stats\n",
    "\t\tsave_dict['reg_loss'] = sess.run(model._reg_loss)\t        \t        \n",
    "\t\treturn save_dict\n",
    "\n",
    "\tdef _choose_model(self, model, reg_ind):\n",
    "\t\t\"\"\" This function adjusts the hyper-parameters based on the dataset / model to be fitted.\"\"\"\n",
    "\t\tself._params['form_train_op'] = False\t\t\t\n",
    "\t\tself._optim_fun = self._ls_optimizer\n",
    "\t\tself._optim_reg = None\n",
    "\t\tif model == 'rf':\n",
    "\t\t\tself._optim_fun = self._rf_optimizer\n",
    "\t\t\tmodel_fn = RF\n",
    "\t\t\tif self._dataset == 'SYNTH':\t\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-5, 2, num=10)\n",
    "\t\t\telif self._dataset in ['CIFAR10.Gray', 'FMNIST', 'NFMNIST']:\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-5, 3, num=20)\n",
    "\t\t\t\tself._params['form_train_op'] = True\n",
    "\t\t\t\tself._optim_fun = self._fc_optimizer\n",
    "\t\t\t\tself._optim_reg = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-4, 4, num=20)\n",
    "\t\telif model == '2layerNTK':\n",
    "\t\t\tmodel_fn = TwoLayerReluNT\n",
    "\t\t\tif self._dataset == 'SYNTH':\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-4, 2, num=10)\n",
    "\t\t\telif self._dataset in ['CIFAR10.Gray', 'FMNIST', 'NFMNIST']:\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-5, 3, num=20)\n",
    "\t\t\t\tself._params['form_train_op'] = True\n",
    "\t\t\t\tself._optim_fun = self._fc_optimizer\n",
    "\t\t\t\tself._optim_reg = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-4, 4, num=20)\n",
    "\t\telif model == 'Myrtle':\n",
    "\t\t\treg_list = 10 ** np.linspace(-5, -2, num=10)\n",
    "\t\t\tself._params['form_train_op'] = True\n",
    "\t\t\tself._optim_fun = self._fc_optimizer\n",
    "\t\t\tmodel_fn = Myrtle\n",
    "\t\t\tself._optim_reg = 0\n",
    "\t\telif model == 'FullyConnected':\n",
    "\t\t\tif self._dataset == 'SYNTH':\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-8, -5, num=15)\n",
    "\t\t\t\treg_list = np.concatenate([reg_list, 10 ** np.linspace(-4.8, -4, num=5)])\n",
    "\t\t\t\treg_list = np.concatenate([reg_list, 10 ** np.linspace(-3.8, -2, num=5)])\n",
    "\t\t\telif self._dataset == 'CIFAR10.Gray':\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-5, -1, num=20)\n",
    "\t\t\telif self._dataset in ['FMNIST', 'NFMNIST']:\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-6, -2, num=20)\n",
    "\t\t\t\tif self._m == 2:\n",
    "\t\t\t\t\treg_list = 10 ** np.linspace(-7, -5, num=10) # Shuffle MNIST 2 layer\n",
    "\t\t\t\t\tprint('Hyper-parameters Chosen for two hidden layers')\n",
    "\t\t\telse:\n",
    "\t\t\t\treg_list = 10 ** np.linspace(-6, -2, num=20)\n",
    "\t\t\tself._params['form_train_op'] = True\n",
    "\t\t\tself._optim_fun = self._fc_optimizer\n",
    "\t\t\tmodel_fn = FullyConnected\n",
    "\t\t\tself._optim_reg = 0\t\t\n",
    "\t\telse:\n",
    "\t\t\traise Exception('Model is not recognized')\t\t\n",
    "\t\t# Choose the regularization format\n",
    "\t\tif self._optim_reg is None:\n",
    "\t\t\tself._optim_reg = reg_list[reg_ind]\n",
    "\t\t\tself._model_reg = 0\n",
    "\t\telse:\n",
    "\t\t\tself._optim_reg = 0\n",
    "\t\t\tself._model_reg = reg_list[reg_ind] * 2.0\n",
    "\t\tprint('Regularization adjusted to %f'%(self._optim_reg + self._model_reg), file=self._file)\n",
    "\t\treturn model_fn\n",
    "\t\t\t\n",
    "\tdef _mkdir(self, exp_name, model_name, job_id, num_units, dataset, exp_flag):\n",
    "\t\tuser_dirs = {}\n",
    "\t\twith open(\"./directories.txt\") as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\t(key, val) = line.split()\n",
    "\t\t\t\tuser_dirs[key] = val\n",
    "\t\tnoise_id = exp_flag.noise_ind\n",
    "\t\tself._directory = user_dirs['nn_dir'] + '%s_%s_%d_%s_%d_%d'%(exp_name, model_name, num_units, dataset, noise_id, job_id)\n",
    "\t\tif not os.path.exists(self._directory):\n",
    "\t\t\tos.makedirs(self._directory)\n",
    "\t\t# Printing the arguments to the log-file\n",
    "\t\tfileName = self._directory + \"/\" + 'log_file.txt'\n",
    "\t\tself._file = open(fileName, 'w', buffering=1)\n",
    "\t\tprint('Arguments:', file=self._file)\n",
    "\t\tprint(exp_flag, file=self._file)\n",
    "\t\ttemp = exp_flag.flag_values_dict()\n",
    "\t\tprint(temp, file=self._file)\n",
    "\t\tprint('=========', file=self._file)\n",
    "\n",
    "\n",
    "experiment = Experiment()\n",
    "print('The Experiment is Finished!')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
